#!/usr/bin/python3
# -*- coding: utf-8 -*-

import argparse
import glob
from itertools import combinations
import logging
import os
import subprocess
import time
import sys
from tqdm import tqdm
import imageio
from scipy.signal import argrelmax
import numpy as np
import cProfile
import pstats
from typing import Iterable, IO, List, Tuple, Any
import warnings
import yaml

from viddup import DB, get_db, FileInfo, get_index, installed_knn_libs  # noqa: F401  FileInfo needed by yaml Loader
from viddup import vidhash
from viddup.tools import format_duration, TqdmStream, NCOLS, whitelist, handle_purge
from viddup.ui import VidDupUI


log_stream = TqdmStream(sys.stdout)


# send imageio warnings to log stream
def showwarning(message, category, filename, lineno, file=None, line=None):
    text = warnings.formatwarning(message, category, filename, lineno, line)
    log_stream.write(text)


warnings.showwarning = showwarning

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)-15s;%(levelname)s;%(message)s',
                    stream=log_stream)

KNOWN_VID_TYPES_DEFAULT = "mp4, mkv, avi, ts, asf, wmv"
INDEX_DIST = 10  # seconds


def get_files(basedir: str, vid_ext: Iterable[str]):
    for root, _, files in os.walk(basedir):
        files.sort()
        for f in files:
            now = time.time()
            _, ext = os.path.splitext(f)
            if ext.lower().lstrip(".") in vid_ext:
                fname = os.path.abspath(os.path.join(root, f))
                # touch files only if they are old enough
                statres = os.stat(fname)
                if now - statres.st_mtime > 36:
                    yield fname


def fix_duration(vidname: str) -> None:
    wd = os.path.dirname(vidname)
    if os.access(vidname, os.W_OK) and os.access(wd, os.W_OK):
        with open(vidname, "rb") as f:
            os.remove(vidname)
            proc = subprocess.Popen(["ffmpeg", "-i", "pipe:",
                                     "-vcodec", "copy",
                                     "-acodec", "copy", vidname], stdin=f)
            proc.wait()
    else:
        logging.warning("target %s is not writable, giving up", vidname)


def get_hashes(vidname, fix=True) -> Tuple[List[Tuple[int, float]], float, float, List[float]]:
    video = imageio.get_reader(vidname, vidhash.NAME)
    md = video.get_meta_data()
    fps = md["fps"]
    nframes = md["nframes"]
    if ("duration" not in md or md["duration"] > 3 * 3600) and fix:
        fix_duration(vidname)
        logging.info("Duration of %s hopefully fixed", vidname)
        return get_hashes(vidname, False)

    duration = md["duration"]

    logging.info("Hashing %s %2.2ff/s: %s", format_duration(duration), fps, vidname)

    brightness = []
    try:
        with tqdm(video.iter_data(), total=nframes, leave=False, ascii=True, ncols=NCOLS) as pb:
            for frame in pb:
                brightness.append(frame.mean())
    except KeyboardInterrupt:
        raise
    except imageio.core.format.CannotReadFrameError:
        pass
    except Exception as e:
        logging.warning("Error processing video: %s", e, exc_info=True)

    extrema = get_extrema(brightness, INDEX_DIST, fps)
    return extrema, fps, duration, brightness


def get_extrema(hashes: List[float], min_dist: float, fps: float) -> List[Tuple[int, float]]:
    """Take the list of brightness values per frame and compute
    a list of pairs (frame number and distance[s] of local maxima)"""

    order = int(min_dist * fps)
    idx = argrelmax(np.array(hashes), order=order)[0]
    result = []
    old_idx = 0
    for i in idx:
        result.append((int(i), (i - old_idx)/fps))
        old_idx = i
    return result


def import_file(dbi: DB, params: argparse.Namespace, fname: str) -> None:
    """Compute and import hashes of the given filename into the database"""

    if not os.access(fname, os.R_OK):
        logging.error("File %s not found", fname)
        return

    fname = os.path.abspath(fname)

    with dbi.getconn() as conn:
        if dbi.is_name_in_db(conn, fname):
            if not params.refresh:
                logging.info("File %s already imported", fname)
                return

    start_time = time.time()
    with dbi.getconn() as conn:
        try:

            # make sure disks have spun up
            with open(fname, "rb") as f:
                f.read(1024)

            index_info, fps, duration, brightness = get_hashes(fname, params.fixduration)

            fi = dbi.insert_file(conn, fname, fps, duration)

            dbi.insert_hashes(conn, fi.fid, index_info)

            dbi.insert_brightness(conn, fi.fid, brightness)

            logging.info("File %s imported in %s",
                         fname,
                         format_duration(time.time() - start_time))
            conn.commit()
        except KeyboardInterrupt:
            logging.warning("Aborted")
            raise
        except Exception:
            logging.warning("Failed to insert hashes for %s", fname, exc_info=True)


def handle_import(dbi: DB, params: argparse.Namespace) -> None:
    if params.dir:
        for fname in get_files(params.dir,
                               params.vidext.replace(" ", "").split(",")):
            import_file(dbi, params, fname)
    if params.file:
        for f in glob.glob(params.file):
            import_file(dbi, params, f)


def handle_search(dbi: DB, params: argparse.Namespace) -> None:
    idx = get_index(dbi, params)
    with dbi.getconn() as conn:
        duplicates = idx.search(conn)

    if len(duplicates) == 0:
        logging.info("No candidates found, giving up")
        return

    if params.searchres:
        with open(params.searchres, "w") as f:
            yaml.dump(duplicates, f, indent=2)

    if params.ui:
        ui = VidDupUI(dbi, params, duplicates)
        ui.start()
    else:
        for match in duplicates:
            logging.info("Group of %d files found", len(match))
            for row in match:
                fileinfo, offset = row
                logging.info("ffplay -ss %s '%s'",
                             format_duration(offset), fileinfo.name)


def fetch_duplicates(dbi: DB, f: IO) -> List[List[List[Any]]]:
    candidates = yaml.load(f, Loader=yaml.Loader)
    result = []
    with dbi.getconn() as conn:
        for row in candidates:
            new_row = []
            for fi, pos in row:
                if os.access(fi.name, os.R_OK):
                    new_row.append([fi, pos])
            if len(new_row) < 2:
                continue

            fids = [fi[0].fid for fi in new_row]
            fids.sort()
            is_whitelisted = True
            for id1, id2 in combinations(fids, 2):
                if not dbi.is_whitelisted(conn, id1, id2):
                    is_whitelisted = False
                    break
            if not is_whitelisted:
                result.append(new_row)

    return result


def handle_searchres(dbi: DB, params: argparse.Namespace) -> None:
    with open(params.searchres) as f:
        duplicates = fetch_duplicates(dbi, f)

    if params.ui:
        ui = VidDupUI(dbi, params, duplicates)
        ui.start()
    else:
        for match in duplicates:
            logging.info("Group of %d files found", len(match))
            for row in match:
                fileinfo, offset = row
                logging.info("ffplay -ss %s '%s'", format_duration(offset), fileinfo.name)


def fixfilenames(args: argparse.Namespace) -> None:
    for fname in get_files(args.dir, args.vidext.replace(" ", "").split(",")):
        try:
            fname.encode("utf8")
        except Exception:
            logging.warn("invalid filename %s", fname, exc_info=True)
            fn = fname.encode("utf8", "ignore")
            new_fname = fn.decode("utf8")
            logging.info("changed into %s", new_fname)
            os.rename(fname, new_fname)


def fix_moved_files(dbi: DB, args: argparse.Namespace) -> None:
    logging.info("scanning for moved files")
    with dbi.getconn() as conn:
        file_infos = dbi.get_file_infos(conn)
    basenames = dict()
    duplicates = set()
    for fi in file_infos:
        bn = os.path.basename(fi.name)
        if bn in basenames:
            duplicates.add(bn)
            logging.info("ambiguous basename for %s", bn)
        basenames[bn] = fi
    for bn in duplicates:
        del basenames[bn]
    with dbi.getconn() as conn:
        for fname in get_files(args.dir, args.vidext.replace(" ", "").split(",")):
            bn = os.path.basename(fname)
            if bn in basenames and fname != basenames[bn].name:
                logging.info("Move detected: %s => %s", basenames[bn].fid, fname)
                dbi.update_name(conn, basenames[bn].fid, fname)
        conn.commit()


def rename_files(dbi: DB, params: argparse.Namespace) -> None:
    old_name, new_name = map(os.path.abspath, params.rename)
    logging.info("Renaming %s -> %s", old_name, new_name)
    if not os.access(old_name, os.R_OK):
        logging.error("File %s not readable, ignoring", old_name)
        return
    if os.access(new_name, os.R_OK):
        logging.error("File %s already exists, ignoring", new_name)
        return
    try:
        with open(new_name, "wb"):
            os.remove(new_name)
    except FileNotFoundError:
        logging.error("File %s is not writable, ignoring", new_name)

    os.rename(old_name, new_name)
    with dbi.getconn() as conn:
        fid = dbi.get_id(conn, str(old_name))
        if not fid:
            logging.warning("File %s not indexed, no DB update needed", old_name)
        else:
            dbi.update_name(conn, fid, str(new_name))
            conn.commit()


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--purge",
                        default=False,
                        action="store_true",
                        help="Purge deleted files from database (dry run mode)")
    parser.add_argument("--delete",
                        default=False,
                        action="store_true",
                        help="Really delete from database in purge")
    parser.add_argument("--nice",
                        type=int,
                        help="Nice level for background operation, default 5",
                        default=5)
    parser.add_argument("--dir",
                        help="Import video hashes from directory and its subdirectories")
    parser.add_argument("--file",
                        help="Import video hashes for a single file or a glob like '*.mp4'")
    parser.add_argument("--refresh",
                        action="store_true",
                        help="Re-hash file but keep whitelistings intact",
                        default=False)
    parser.add_argument("--search",
                        action="store_true",
                        help="Search duplicates in database")
    parser.add_argument("--ignore_start",
                        type=int,
                        default=0,
                        help="Ignore search results startin in"
                        " the first seconds of a movie, default 0")
    parser.add_argument("--ignore_end",
                        type=int,
                        default=0,
                        help="Ignore search results starting in the last"
                        " seconds of a movie, default 0")
    parser.add_argument("--dbhost",
                        help="PostgreSQL database server host",
                        default="gateway")
    parser.add_argument("--dbport",
                        type=int,
                        help="PostgreSQL database server port",
                        default=5432)
    parser.add_argument("--dbname",
                        help="PostgreSQL database name",
                        default="viddup")
    parser.add_argument("--dbuser",
                        help="PostgreSQL database user name",
                        default="viddup")
    parser.add_argument("--dbpwd",
                        help="PostgreSQL database user password",
                        default="viddup")
    parser.add_argument("--indexlength",
                        default=10,
                        type=int,
                        help="Length of index in searches, smaller"
                        " values will produce more false positives, default 10")
    parser.add_argument("--scenelength",
                        default=300,
                        type=int,
                        help="Length in seconds of scenes to match, default 300")
    parser.add_argument("--radius",
                        default=3.0,
                        type=float,
                        help="Measure for acceptable index difference. Higher"
                        " values will result in more false positives, default 3.0")
    parser.add_argument("--ui",
                        action="store_true",
                        help="Launch ui after search")
    parser.add_argument("--searchres",
                        help="Filename of search result, used"
                        " in --search and --ui without --search")
    parser.add_argument("--step",
                        type=int,
                        default=1,
                        help="Step width for searching index, default 1")
    parser.add_argument("--whitelist",
                        nargs='+',
                        help="Whitelist a list of files")
    parser.add_argument("--knnlib",
                        default=None,
                        help="knn library to use, choose " + ", ".join(installed_knn_libs()))
    parser.add_argument("--vidext",
                        default=KNOWN_VID_TYPES_DEFAULT,
                        help="filename extensions to consider,"
                        " default %s" % KNOWN_VID_TYPES_DEFAULT)
    parser.add_argument("--fixduration",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--fixrenames",
                        default=False,
                        action="store_true",
                        help="try to fix database filenames for moved files")
    parser.add_argument("--rename",
                        nargs=2,
                        help="rename files in filesystem and database")
    parser.add_argument("--fixspeed",
                        default=False,
                        action="store_true",
                        help="Make search more robust for time-scaled videos, "
                        "increase indexlength to reduce number of false positives")
    parser.add_argument("--fixfilenames",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--debug",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--profile",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)

    params = parser.parse_args()

    try:
        nl = os.nice(0)
        nl = os.nice(max(params.nice - nl, 0))
        logging.info("Nice level %d", nl)
    except Exception:
        logging.info("Setting nice level not supported")

    dbi = get_db(params)

    if params.profile:
        profile = cProfile.Profile()
        profile.enable()

    if params.fixrenames:
        if params.dir:
            fix_moved_files(dbi, params)
        else:
            logging.error("please set --dir option as well")
            sys.exit(1)

    if params.rename:
        rename_files(dbi, params)

    if params.fixfilenames:
        if params.dir:
            fixfilenames(params)
        else:
            logging.error("please set --dir option as well")
            sys.exit(1)

    if params.whitelist:
        whitelist(dbi, params)

    if params.purge:
        handle_purge(dbi, params)

    if params.dir or params.file:
        handle_import(dbi, params)

    if params.search:
        handle_search(dbi, params)
    elif params.searchres:
        handle_searchres(dbi, params)

    if params.profile:
        profile.create_stats()
        s = pstats.Stats(profile)
        s.sort_stats("cumulative")
        s.print_stats(50)
