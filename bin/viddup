#!/usr/bin/python3

import argparse
import bisect
from itertools import combinations
import importlib
import logging
import os
from pprint import pprint
import sqlite3 as db
import subprocess
import time
import types
import sys
from tqdm import tqdm
import gi
from gi.repository import GObject
gi.require_version('Gtk', '3.0')
from gi.repository import Gtk
import imageio
from viddup import vidhash
from scipy.signal import argrelmax
import numpy as np
import cProfile
import pstats
import multiprocessing
from collections import namedtuple

class TqdmStream:

    def __init__(self, stream):
        self.stream = stream
    
    def write(self, data):
        data = data.rstrip("\r\n")
        if data:
            tqdm.write(data, file=self.stream)
            self.stream.flush()

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)-15s;%(levelname)s;%(message)s',
                    stream=TqdmStream(sys.stdout))

DB_URI_DEFAULT = "viddup.sqlite3"
KNOWN_VID_TYPES_DEFAULT = "mp4, mkv, avi, ts, asf"
INDEX_DIST = 10 # seconds
ANN_LIBS = []

for module_name in [ "hnswlib", "cyflann", "annoy" ]:
    try:
        importlib.import_module(module_name)
        ANN_LIBS.append(module_name)
    except ModuleNotFoundError:
        pass

if not ANN_LIBS:
    logging.error("Please install at least one of the k-nearest neighbor libraries: hnswlib, cyflann, or annoy")
    sys.exit(1)

FileInfo = namedtuple("FileInfo", "fid, name, fps, duration")
    
class Index:

    def __init__(self, params):
        logging.info("Using knn library %s", params.knnlib)
        self.params = params
        self.knn_module = importlib.import_module(params.knnlib)

        self.mk_index = getattr(self, "mk_idx_%s" % params.knnlib)
        self.idx_get_nn = getattr(self, "idx_get_nn_%s" % params.knnlib)
        self.idx_get_length = getattr(self, "idx_get_length_%s" % params.knnlib)
        self.idx_get_row = getattr(self, "idx_get_row_%s" % params.knnlib)
        
        self.init_index()

    def search(self):
        logging.info("Searching duplicates")
        radius = self.params.radius
        step = self.params.step
        debug = self.params.debug
        data_length = self.idx_get_length()

        known_duplicates = set()
        result = []
        
        for i in tqdm(range(0, data_length, step)):
            elem_idx = self.idx_get_nn(i, radius)
            elem_idx.sort()
            if len(elem_idx) > 1:
                details = []

                fids = list(set(self.get_fids_by_items(elem_idx)))
                fids.sort()
                pairs = list(combinations(fids, 2))

                for pair in pairs[:]:
                    if pair in known_duplicates or is_whitelisted(pair):
                        pairs.remove(pair)
                fids = set(i[0] for i in pairs).union(i[1] for i in pairs)

                if not fids:
                    continue
                
                known_duplicates.update(set(pairs))

                known_fids = set()
                for n, item in enumerate(elem_idx):
                    try:
                        start_item, fid = self.get_fid_by_item(item)
                        if fid not in fids or fid in known_fids:
                            continue
                        known_fids.add(fid)
                        fileinfo, frame = self.get_frame(fid, item - start_item)
                        if debug:
                            logging.info("%4d, %-50s: %s", fileinfo.fid, fileinfo.name[-50:], self.idx_get_row(item))
                        details.append([fileinfo, frame/fileinfo.fps])
                    except:
                        logging.info("Error processing: %s, purge required?", item, exc_info=True)
                result.append(details)
        return result
        

    def init_index(self):
        logging.info("Loading hashes")
        self.fid_list = []
        self.item_list = []
        item_num = 0
        index_length = self.params.indexlength
        scene_seconds = self.params.scenelength
        items = []
        for fileinfo in self.get_file_infos():
            hashes = self.get_hashes(fileinfo.fid)
            item_count = max(0, len(hashes) - index_length)
            self.fid_list.append(fileinfo.fid)
            self.item_list.append(item_num)
            for i in range(item_count):
                item = hashes[i:i+index_length]
                total_time = 0
                for n, v in enumerate(item):
                    if total_time > scene_seconds:
                        item[n] = 0.0
                    total_time += v
                items.append(item)
            item_num += item_count
        self.mk_index(items)
                        
    def get_fid_by_item(self, item):
        pos = max(0, bisect.bisect_right(self.item_list, item) - 1)
        return self.item_list[pos], self.fid_list[pos]

    def get_fids_by_items(self, items):
        r = [ self.get_fid_by_item(item)[1] for item in items ]
        r.sort()
        return r

    def get_file_infos(self):
        with get_db() as db_conn:
            c = db_conn.cursor()
            c.execute("select rowid, name, fps, duration from filenames order by rowid asc")
            for i in tqdm(c.fetchall()):
                yield FileInfo._make(i)
            c.close()

    def get_frame(self, fid, pos):
        with get_db() as db_conn:
            c = db_conn.cursor()
            c.execute("select id, name, fps, duration from filenames where id = ?", [fid])
            fileinfo = FileInfo._make(c.fetchone())
            c.execute("select frame from hashes where filename_id = ? order by frame limit 1 offset ?", [fid, int(pos)])
            frame = c.fetchone()[0]
            c.close()

        return (fileinfo, frame)

    def get_hashes(self, fid):
        with get_db() as db_conn:
            c = db_conn.cursor()
            c.execute("select hash from hashes where filename_id = ? order by frame", [fid])
            hashes = [p[0] for p in c]
            c.close()
            return hashes
            
    # annoy support, c.f. https://github.com/spotify/annoy
    def mk_idx_annoy(self, items):
        logging.info("Start building annoy index")
        self.idx = self.knn_module.AnnoyIndex(self.params.indexlength, metric="euclidean")
        for n, item in enumerate(items):
            self.idx.add_item(n, item)
        self.idx.build(20)

    def idx_get_length_annoy(self):
        return self.idx.get_n_items()

    def idx_get_nn_annoy(self, rownum, radius):
        result = []
        count=20
        elem_idx, elem_dists = self.idx.get_nns_by_item(rownum, count, include_distances=True)
        for n, item in enumerate(elem_idx):
            if elem_dists[n] < radius:
                result.append(item)
        return result

    def idx_get_row_annoy(self, rownum):
        return self.idx.get_item_vector(rownum)
    
    # flann support, c.f. https://github.com/dougalsutherland/cyflann
    def mk_idx_cyflann(self, items):
        logging.info("Start building flann index")
        self.idx = self.knn_module.FLANNIndex(algorithm="kdtree")
        self.idx.build_index(items)

    def idx_get_length_cyflann(self):
        return len(self.idx.data)

    def idx_get_nn_cyflann(self, rownum, radius):
        row = self.idx.data[rownum]
        elem_idx, _ = self.idx.nn_radius(row, radius, sorted=True)
        return elem_idx

    def idx_get_row_cyflann(self, rownum):
        return self.idx.data[rownum]

    # hnswlib support, c.f. https://github.com/nmslib/hnsw
    def mk_idx_hnswlib(self, items):
        logging.info("Start building hnswlib index")
        index_length = self.params.indexlength
        self.idx = self.knn_module.Index(space='l2', dim=index_length)
        self.idx.set_num_threads(multiprocessing.cpu_count())
        self.idx.init_index(max_elements=len(items), ef_construction=100, M=index_length)
        self.idx.add_items(np.array(items))
        self.__items = items

    def idx_get_length_hnswlib(self):
        return len(self.__items)

    def idx_get_nn_hnswlib(self, rownum, radius):
        result = []
        elem_idx, elem_dists = self.idx.knn_query(self.__items[rownum], k=20)
        for n, item in enumerate(elem_idx[0]):
            if elem_dists[0][n] < radius:
                result.append(item)
        return result

    def idx_get_row_hnswlib(self, rownum):
        return self.__items[rownum]

def get_db():
    conn = db.connect(DB_URI)
    conn.execute("pragma busy_timeout = 300000").close()
    return conn

def get_files(basedir, vid_ext):
    for root, _, files in os.walk(basedir):
        for f in files:
            _, ext = os.path.splitext(f)
            if ext.lower().lstrip(".") in vid_ext:
                yield os.path.join(root, f)

def get_hashes(vidname):
    video = imageio.get_reader(vidname, "vidhash")
    md = video.get_meta_data()
    fps = md["fps"]
    nframes = md["nframes"]
    duration = md["duration"]
    logging.info("Hashing %s %2.2ff/s: %s", format_duration(duration), fps, vidname)
    
    brightness = []
    try:
        for frame in tqdm(video.iter_data(), total=nframes, leave=False):
            brightness.append(frame.mean())
    except KeyboardInterrupt:
        raise
    except imageio.core.format.CannotReadFrameError:
        pass
    except Exception as e:
        logging.warning("Error processing video: %s", e, exc_info=True)

    extrema = get_extrema(brightness, INDEX_DIST, fps)
    return extrema, fps, duration

def get_extrema(hashes, min_dist, fps):
    """Take the list of brightness values per frame and compute 
    a list of pairs (frame number and local maximum)"""
    
    order = int(min_dist * fps)
    idx = argrelmax(np.array(hashes), order=order)[0]
    result = []
    old_idx = 0
    for i in idx:
        result.append((int(i), (i - old_idx)/fps))
        old_idx = i
    return result


def is_whitelisted(ids):
    """Return True, if all pairs of filenames are whitelisted"""
    try:
        #ids = get_ids(fnames)
        with get_db() as db_conn:
            c = db_conn.cursor()
            for id1, id2 in combinations(ids, 2):
                if id1 > id2:
                    id1, id2 = id2, id1
                c.execute("select 1 from whitelist where id1 = ? and id2 = ?", (id1, id2))
                if c.fetchone() is None:
                    return False
            c.close()
            return True
    except Exception as e:
        logging.warning(e)
        return False

def make_schema():
    """Create the database schema if it does not exist already"""
    
    with get_db() as db_conn:
        db_conn.execute("create table if not exists filenames (id INTEGER PRIMARY KEY, name text, fps float, duration float)").close()
        db_conn.execute("create unique index if not exists name_ux on filenames (name)").close()
        db_conn.execute("create table if not exists hashes (filename_id int64, frame int, hash float)").close()
        db_conn.execute("create table if not exists whitelist (id1 INTEGER, id2 INTEGER)").close()
        db_conn.execute("create unique index if not exists whitelist_ux on whitelist (id1, id2)").close()
        db_conn.execute("create unique index if not exists filename_id_frame_ux on hashes (filename_id, frame)").close()

def import_file(fname):
    """Compute and import hashes of the given filename into the database"""
    start_time = time.time()
    with get_db() as db_conn:
        try:
            c = db_conn.cursor()
            c.execute("select 1 from filenames where name = ?", [fname])
            if c.fetchall():
                logging.info("File %s already imported", fname)        
                return
            hashes, fps, duration = get_hashes(fname)

            c.execute("insert into filenames values (null, ?, ?, ?)", [fname, fps, duration])
            rowid = c.lastrowid

            for frame, h in hashes:
                c.execute("insert into hashes values (?, ?, ?)", [ rowid, frame, h ])
            c.close()
            db_conn.commit()
            logging.info("File %s imported in %s", fname, format_duration(time.time() - start_time))
        except KeyboardInterrupt:
            logging.warning("Aborted")
            raise
        except:
            logging.warning("Failed to insert hashes for %s", fname, exc_info=True)
        db_conn.rollback()

def tidy_db(delete):

    with get_db() as db_conn:
        c = db_conn.cursor()
        c.execute("select distinct filename_id from hashes where filename_id not in (select rowid from filenames)")
        for fid, in c:
            db_conn.execute("delete from hashes where filename_id = ?", (fid,))
        
    with get_db() as db_conn:
        del_files = []
        file_count = 0
        c = db_conn.cursor()
        c.execute("select name, rowid from filenames")
        for fn, rowid in c:
            file_count += 1
            if not os.access(fn, os.R_OK):
                del_files.append((rowid, fn))
        c.close()
        
    if delete:
        with get_db() as db_conn:
            c = db_conn.cursor()
            c.execute("delete from filenames where not exists (select 1 from hashes where filename_id = id)")
            c.execute("delete from hashes where filename_id not in (select id from filenames)")
            for rowid, _ in del_files:
                c.execute("delete from hashes where filename_id = ?", (rowid,))
                c.execute("delete from filenames where rowid = ?", (rowid,))
                c.execute("delete from whitelist where id1 = ? or id2 = ?", (rowid, rowid))
                
            c.close()
    else:
        logging.warning("Need to delete %d of %d files", len(del_files), file_count)
        for _, fn in del_files:
            logging.info(fn)

def get_ids(files):
    ids = []
    with get_db() as db_conn:
        c = db_conn.cursor()
        for f in files:
            try:
                c.execute("select id from filenames where name = ?", (f,))
                ids.append(c.fetchone()[0])
            except:
                raise Exception("File '%s' not known" % f)
        c.close()
    ids.sort()
    return ids

def whitelist(files):
    if len(files) < 2:
        logging.warning("Need at least two files to whitelist")
        return

    try:
        ids = get_ids(files)
    except Exception as e:
        logging.warning(e)
        return

    retval = []
    with get_db() as db_conn:
        c = db_conn.cursor()
        for id1, id2 in combinations(ids,2):
            try:
                if id1 > id2:
                    id1, id2 = id2, id1
                retval.append((id1, id2))
                c.execute("insert into whitelist values (?,?)", (id1, id2))
                db_conn.commit()
            except:
                db_conn.rollback()
        c.close()
    return retval

def format_duration(seconds):
    return time.strftime("%H:%M:%S", time.gmtime(seconds))
        
class VidDupUI(object):

    def __init__(self, duplicates):
        self.duplicates = duplicates

    def start(self):
        self.builder = Gtk.Builder()
        self.builder.add_from_file("/usr/share/viddup/viddup.glade")
        
        window = self.builder.get_object("app")

        data_grid = Gtk.Grid()
        data_grid.insert_column(0)
        data_grid.insert_column(0)
        data_grid.insert_column(0)
        rows = 0
        for index, group in enumerate(self.duplicates):
            for row in group:
                fileinfo, offset = row
                fn = Gtk.Label(label=self.make_label(fileinfo), xalign=0, xpad=10)
                row.append(fn)
                offset = Gtk.Label(label=format_duration(offset), xalign=0, xpad=10)
                delete = Gtk.Button(label="delete")
                delete.connect("clicked", self.onDelete, fileinfo.name)
                data_grid.insert_row(rows)
                data_grid.attach(fn, 0, rows, 1, 1)
                data_grid.attach(offset, 1, rows, 1, 1)
                data_grid.attach(delete, 2, rows, 1, 1)
                rows += 1
            box = Gtk.Box(orientation="horizontal")
            play = Gtk.Button(label="play")
            play.connect("clicked", self.onPlay, index)
            box.pack_start(play, expand=False, fill=True, padding=0)
            whitelist = Gtk.Button(label="whitelist")
            whitelist.connect("clicked", self.onWhitelist, index)
            box.pack_start(whitelist, expand=False, fill=True, padding=0)
            box.pack_end(Gtk.Label(), expand=True, fill=True, padding=0)
            data_grid.insert_row(rows)
            data_grid.attach(box, 0, rows, 3, 1)
            rows += 1
            
        main_area = self.builder.get_object("main_area")
        main_area.add(data_grid)

        self.builder.connect_signals(self)
        
        window.show_all()
        
        Gtk.main()

    def onPlay(self, *args):
        index = args[1]
        procs = []
        for fileinfo, offset, _ in self.duplicates[index]:
            if os.access(fileinfo.name, os.R_OK):
                procs.append(subprocess.Popen(["ffplay", "-sn", "-ss", format_duration(offset), fileinfo.name],
                                                stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL))

        finished = not bool(procs)
        while not finished:
            for p in procs:
                if (0,0) != os.waitpid(p.pid, os.WNOHANG):
                    finished = True
                    break
            time.sleep(0.2)
            
        for p in procs:
            try:
                p.kill()
            except: pass

    def onWhitelist(self, *args):
        args[0].destroy()
        index = args[1]
        fnames = [d[0].name for d in self.duplicates[index]]
        logging.info("Whitelisting %s", fnames)
        pairs = whitelist(fnames)

    def onDelete(self, *args):
        fname = args[1]
        try:
            os.remove(fname)
            logging.info("Deleted %s", fname)
            for group in self.duplicates:
                for fileinfo, offset, label in group:
                    if fname == fileinfo.name:
                        label.set_markup("<span strikethrough='true'>%s</span>" % GObject.markup_escape_text(self.make_label(fileinfo)))
                        label.set_use_markup(True)
        except Exception as e:
            logging.error("Failed to delete %s: %s", fname, e)
        
    def onQuit(self, *args):
        Gtk.main_quit(*args)

    def onPurge(self, *args):
        switch = self.builder.get_object("delete_switch")
        tidy_db(switch.get_active())

    def make_label(self, fileinfo):
        return format_duration(fileinfo.duration) + " " + fileinfo.name

def handle_import(args):
    for fname in get_files(args.dir, args.vidext.replace(" ","").split(",")):
        import_file(fname)

def handle_search(params):
    idx = Index(params)
    duplicates = idx.search()

    if params.ui:
        ui = VidDupUI(duplicates)
        ui.start()
    else:
        for match in duplicates:
            logging.info("Group of %d files found", len(match))
            for row in match:
                fileinfo, offset = row
                logging.info("ffplay -ss %s '%s'", format_duration(offset), fileinfo.name)

def fixfileinfo(params):
    with get_db() as db_conn:
        c = db_conn.cursor()
        c.execute("select id, name, fps, duration from filenames order by id asc")
        for i in tqdm(c.fetchall()):
            fid, name, fps, duration = i
            if fps and duration:
                continue
            logging.info("fetching metadata for %s", name)
            video = imageio.get_reader(name, "vidhash")
            md = video.get_meta_data()
            fps = md["fps"]
            duration = md["duration"]
            c.execute("update filenames set duration=?, fps=? where rowid=?", (duration, fps, fid))
            db_conn.commit()
        c.close()

if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--purge", default=False, action="store_true", help="Purge deleted files from database (dry run mode)")
    parser.add_argument("--delete", default=False, action="store_true", help="Really delete from database in purge")
    parser.add_argument("--dir", help="Import video hashes from directory and its subdirectories")
    parser.add_argument("--search", action="store_true", help="Search duplicates in database")
    parser.add_argument("--db", help="Database file", default=DB_URI_DEFAULT)
    parser.add_argument("--indexlength", default=10, type=int, help="Length of index in searches, smaller values will produce more false positives, default 10")
    parser.add_argument("--scenelength", default=300, type=int, help="Length in seconds of scenes to match, default 300")
    parser.add_argument("--radius", default=3.0, type=float, help="Measure for acceptable index difference. Higher values will result in more false positives, default 3.0")
    parser.add_argument("--ui", action="store_true", help="Launch ui after search")
    parser.add_argument("--step", type=int, default=1, help="Step width for searching index, default 1")
    parser.add_argument("--whitelist", nargs='+', help="Whitelist a list of files")
    parser.add_argument("--knnlib", default=ANN_LIBS[0], help="knn library to use, choose " + ", ".join(ANN_LIBS))
    parser.add_argument("--vidext", default=KNOWN_VID_TYPES_DEFAULT, help="filename extensions to consider, default %s" % KNOWN_VID_TYPES_DEFAULT)
    parser.add_argument("--fixfileinfo", default=False, action="store_true", help=argparse.SUPPRESS)
    parser.add_argument("--debug", default=False, action="store_true", help=argparse.SUPPRESS)
    parser.add_argument("--profile", default=False, action="store_true", help=argparse.SUPPRESS)

    params = parser.parse_args()

    DB_URI=params.db

    make_schema()

    if params.knnlib not in ANN_LIBS:
        logging.error("Unsupported knn library %s", params.knnlib)
        sys.exit(1)

    if params.profile:
        profile = cProfile.Profile()
        profile.enable()

    if params.fixfileinfo:
        fixfileinfo(params)
    
    if params.whitelist:
        whitelist(params.whitelist)
    
    if params.purge:
        tidy_db(params.delete)

    if params.dir:
        handle_import(params)

    if params.search:
        handle_search(params)

    if params.profile:
        profile.create_stats()
        s = pstats.Stats(profile)
        s.sort_stats("cumulative")
        s.print_stats(50)

