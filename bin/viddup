#!/usr/bin/python3
# coding: utf-8

import argparse
from itertools import combinations
import importlib
import logging
import os
import subprocess
import time
import sys
from tqdm import tqdm
import gi
import imageio
from scipy.signal import argrelmax
import numpy as np
import cProfile
import pstats
import multiprocessing
from collections import namedtuple
import warnings
import yaml

from viddup import get_db, FileInfo  # noqa: F401  FileInfo needed by yaml Loader
from viddup import vidhash  # noqa: F401  importing registers format with imageio

import gi.repository
gi.require_version('Gtk', '3.0')  # noqa: E402
from gi.repository import Gtk, GLib

NCOLS = 70  # columns for progress bar


class TqdmStream:

    def __init__(self, stream):
        self.stream = stream

    def write(self, data):
        data = data.rstrip("\r\n")
        if data:
            tqdm.write(data, file=self.stream)
            self.stream.flush()


log_stream = TqdmStream(sys.stdout)


# send warnings to log stream
def warn_msg_impl(msg):
    text = warnings._formatwarnmsg(msg)
    log_stream.write(text)


warnings._showwarnmsg_impl = warn_msg_impl

logging.basicConfig(level=logging.INFO,
                    format='%(asctime)-15s;%(levelname)s;%(message)s',
                    stream=log_stream)

KNOWN_VID_TYPES_DEFAULT = "mp4, mkv, avi, ts, asf, wmv"
INDEX_DIST = 10  # seconds
ANN_LIBS = []

for module_name in ["hnswlib", "cyflann", "annoy"]:
    try:
        importlib.import_module(module_name)
        ANN_LIBS.append(module_name)
    except ModuleNotFoundError:
        pass

if not ANN_LIBS:
    logging.error("Please install at least one of the k-nearest neighbor "
                  "libraries: hnswlib, cyflann, or annoy")
    sys.exit(1)


class Index:

    def __init__(self, dbi, params):
        logging.info("Using knn library %s", params.knnlib)
        self.dbi = dbi
        self.params = params
        self.knn_module = importlib.import_module(params.knnlib)

        self.mk_index = getattr(self, "mk_idx_%s" % params.knnlib)
        self.idx_get_nn = getattr(self, "idx_get_nn_%s" % params.knnlib)
        self.idx_get_length = getattr(self, "idx_get_length_%s" % params.knnlib)
        self.idx_get_row = getattr(self, "idx_get_row_%s" % params.knnlib)

        self.init_index()

    def search(self):
        logging.info("Searching duplicates")
        radius = self.params.radius
        step = self.params.step
        debug = self.params.debug
        data_length = self.idx_get_length()

        known_duplicates = set()
        result = []

        for i in tqdm(range(0, data_length, step), ascii=True, ncols=NCOLS):
            elem_idx = self.idx_get_nn(i, radius)
            elem_idx.sort()
            if len(elem_idx) > 1:
                details = []

                fids = [self.fi_list[i].fid for i in elem_idx]
                fids.sort()

                pairs = list(combinations(fids, 2))

                for pair in pairs[:]:
                    if pair[0] == pair[1] or pair in known_duplicates or self.is_whitelisted(pair):
                        known_duplicates.add(pair)
                        pairs.remove(pair)
                fids = set(i[0] for i in pairs).union(i[1] for i in pairs)
                if not fids:
                    continue

                known_duplicates.update(set(pairs))
                known_fids = set()
                for n, item in enumerate(elem_idx):
                    try:
                        fid = self.fi_list[item].fid
                        if fid not in fids or fid in known_fids:
                            continue
                        known_fids.add(fid)
                        fileinfo, frame = self.fi_list[item], self.frame_list[item]
                        if debug:
                            logging.info("%4d, %-50s: %s",
                                         fileinfo.fid,
                                         fileinfo.name[-50:],
                                         self.idx_get_row(item))
                        details.append([fileinfo, frame/fileinfo.fps])
                    except KeyboardInterrupt:
                        raise
                    except Exception:
                        logging.info("Error processing: %s, purge required?", item, exc_info=True)
                if len(details) > 1:
                    result.append(details)
        return result

    def is_whitelisted(self, ids):
        """Return True, if all pairs of filenames are whitelisted"""
        for id1, id2 in combinations(ids, 2):
            if not self.dbi.is_whitelisted(id1, id2):
                return False
        return True

    def init_index(self):
        logging.info("Loading hashes")
        self.fi_list = []    # fileinfo for item
        self.frame_list = []  # first frame for item
        items = []

        index_length = self.params.indexlength
        scene_seconds = self.params.scenelength
        ignore_start = self.params.ignore_start
        ignore_end = self.params.ignore_end

        with tqdm(self.dbi.get_file_infos(), ascii=True, ncols=NCOLS) as progress_bar:
            for fileinfo in progress_bar:
                min_frame = int(ignore_start * fileinfo.fps)
                max_frame = int((fileinfo.duration - ignore_end) * fileinfo.fps)

                frames, hashes = dbi.get_hashes(fileinfo.fid, min_frame, max_frame)
                if len(hashes) < 5:  # ignore fid, if list of hashes is too short
                    continue

                item_count = max(0, len(hashes) - index_length)

                for i in range(item_count):
                    item = hashes[i:i+index_length]
                    total_time = 0
                    for n, v in enumerate(item):
                        if total_time > scene_seconds:
                            item[n] = 0.0
                        total_time += v
                    items.append(item)
                    self.fi_list.append(fileinfo)
                    self.frame_list.append(frames[i])

        self.mk_index(items)

    # annoy support, c.f. https://github.com/spotify/annoy
    def mk_idx_annoy(self, items):
        logging.info("Start building annoy index")
        self.idx = self.knn_module.AnnoyIndex(self.params.indexlength,
                                              metric="euclidean")
        if self.params.fixspeed:
            for n, item in enumerate(items):
                item = np.array(item)
                items[n] = list(128 * item / np.mean(item))
        for n, item in enumerate(items):
            self.idx.add_item(n, item)
        self.idx.build(20)

    def idx_get_length_annoy(self):
        return self.idx.get_n_items()

    def idx_get_nn_annoy(self, rownum, radius):
        result = []
        count = 20
        elem_idx, elem_dists = self.idx.get_nns_by_item(rownum,
                                                        count,
                                                        include_distances=True)
        for n, item in enumerate(elem_idx):
            if elem_dists[n] < radius:
                result.append(item)
        return result

    def idx_get_row_annoy(self, rownum):
        return self.idx.get_item_vector(rownum)

    # flann support, c.f. https://github.com/dougalsutherland/cyflann
    def mk_idx_cyflann(self, items):
        logging.info("Start building flann index")
        self.idx = self.knn_module.FLANNIndex(algorithm="kdtree")
        if self.params.fixspeed:
            for n, item in enumerate(items):
                item = np.array(item)
                items[n] = list(128 * item / np.mean(item))
        self.idx.build_index(items)

    def idx_get_length_cyflann(self):
        return len(self.idx.data)

    def idx_get_nn_cyflann(self, rownum, radius):
        row = self.idx.data[rownum]
        elem_idx, _ = self.idx.nn_radius(row, radius, sorted=True)
        return elem_idx

    def idx_get_row_cyflann(self, rownum):
        return self.idx.data[rownum]

    # hnswlib support, c.f. https://github.com/nmslib/hnsw
    def mk_idx_hnswlib(self, items):
        logging.info("Start building hnswlib index")
        index_length = self.params.indexlength
        self.idx = self.knn_module.Index(space='l2', dim=index_length)
        self.idx.set_num_threads(multiprocessing.cpu_count())
        self.idx.init_index(max_elements=len(items), ef_construction=100, M=index_length)
        items = np.array(items)
        if self.params.fixspeed:
            for n, item in enumerate(items):
                items[n] = 128.0*item/np.mean(item)

        self.idx.add_items(items)
        self.__items = items

    def idx_get_length_hnswlib(self):
        return len(self.__items)

    def idx_get_nn_hnswlib(self, rownum, radius):
        result = []
        elem_idx, elem_dists = self.idx.knn_query(self.__items[rownum], k=20)
        for n, item in enumerate(elem_idx[0]):
            if elem_dists[0][n] < radius:
                result.append(item)
        return result

    def idx_get_row_hnswlib(self, rownum):
        return self.__items[rownum]


def get_files(basedir, vid_ext):
    for root, _, files in os.walk(basedir):
        for f in files:
            _, ext = os.path.splitext(f)
            if ext.lower().lstrip(".") in vid_ext:
                yield os.path.abspath(os.path.join(root, f))


def fix_duration(vidname):
    wd = os.path.dirname(vidname)
    if os.access(vidname, os.W_OK) and os.access(wd, os.W_OK):
        with open(vidname, "rb") as f:
            os.remove(vidname)
            proc = subprocess.Popen(["ffmpeg", "-i", "pipe:",
                                     "-vcodec", "copy",
                                     "-acodec", "copy", vidname], stdin=f)
            proc.wait()
    else:
        logging.warning("target %s is not writable, giving up", vidname)


def get_hashes(vidname, fix=True):
    video = imageio.get_reader(vidname, "vidhash")
    md = video.get_meta_data()
    fps = md["fps"]
    nframes = md["nframes"]
    if ("duration" not in md or md["duration"] > 3 * 3600) and fix:
        fix_duration(vidname)
        logging.info("Duration of %s hopefully fixed", vidname)
        return get_hashes(vidname, False)

    duration = md["duration"]

    logging.info("Hashing %s %2.2ff/s: %s", format_duration(duration), fps, vidname)

    brightness = []
    try:
        with tqdm(video.iter_data(), total=nframes, leave=False, ascii=True, ncols=NCOLS) as pb:
            for frame in pb:
                brightness.append(frame.mean())
    except KeyboardInterrupt:
        raise
    except imageio.core.format.CannotReadFrameError:
        pass
    except Exception as e:
        logging.warning("Error processing video: %s", e, exc_info=True)

    extrema = get_extrema(brightness, INDEX_DIST, fps)
    return extrema, fps, duration, brightness


def get_extrema(hashes, min_dist, fps):
    """Take the list of brightness values per frame and compute
    a list of pairs (frame number and local maximum)"""

    order = int(min_dist * fps)
    idx = argrelmax(np.array(hashes), order=order)[0]
    result = []
    old_idx = 0
    for i in idx:
        result.append((int(i), (i - old_idx)/fps))
        old_idx = i
    return result


def import_file(dbi, params, fname):
    """Compute and import hashes of the given filename into the database"""
    start_time = time.time()
    with dbi.transaction() as conn:
        try:
            if dbi.is_name_in_db(fname):
                if not params.refresh:
                    logging.info("File %s already imported", fname)
                    return

            index_info, fps, duration, brightness = get_hashes(fname,
                                                               params.fixduration)

            fi = dbi.insert_file(fname, fps, duration)

            dbi.insert_hashes(fi.fid, index_info)

            dbi.insert_brightness(fi.fid, brightness)

            conn.commit()
            logging.info("File %s imported in %s",
                         fname,
                         format_duration(time.time() - start_time))

        except KeyboardInterrupt:
            logging.warning("Aborted")
            raise
        except Exception:
            logging.warning("Failed to insert hashes for %s", fname, exc_info=True)


def handle_purge(dbi, params, do_delete=None):

    if do_delete is None:
        do_delete = params.delete

    del_files = []
    fn_fis = []
    for fi in dbi.get_file_infos():
        fn_fis.append(fi.fid)
        if not os.access(fi.name, os.R_OK):
            del_files.append(fi)

    logging.warning("Need to delete %d of %d files",
                    len(del_files),
                    len(fn_fis))
    if do_delete:
        for fi in del_files:
            logging.info("deleting %s", fi.name)
            dbi.del_file(fi.fid)
    else:
        for fi in del_files:
            logging.info(fi.name)


def whitelist(dbi, params, files=None):

    Entry = namedtuple("Entry", "name, fid")

    if files is None:
        files = params.whitelist

    with dbi.transaction() as conn:
        ids = set()
        for f in files:
            fid = dbi.get_id(f)
            if fid is not None:
                ids.add(Entry._make([f, fid]))
            else:
                logging.warn("File %s not found in DB", f)

        if len(ids) < 2:
            logging.warning("Need at least two files to whitelist")
            return

        retval = []
        for f1, f2 in combinations(list(ids), 2):
            try:
                dbi.whitelist(f1.fid, f2.fid)
                conn.commit()
                retval.append((f1, f2))
                logging.info("Whitelisted %s and %s" % (f1.name, f2.name))
            except Exception:
                logging.error("Failed to whitelist pair %s - %s", f1.name, f2.name, exc_info=False)
        return retval


def format_duration(seconds):
    return time.strftime("%H:%M:%S", time.gmtime(seconds))


class VidDupUI(object):

    def __init__(self, dbi, params, duplicates):
        self.dbi = dbi
        self.params = params
        self.duplicates = duplicates

    def start(self):
        self.builder = Gtk.Builder()
        self.builder.add_from_file("/usr/share/viddup/viddup.glade")

        window = self.builder.get_object("app")

        data_grid = Gtk.Grid()
        data_grid.insert_column(0)
        data_grid.insert_column(0)
        data_grid.insert_column(0)
        rows = 0
        for index, group in enumerate(self.duplicates):
            for row in group:
                fileinfo, offset = row
                fn = Gtk.Label(label=self.make_label(fileinfo), xalign=0, xpad=10)
                row.append(fn)
                offset = Gtk.Label(label=format_duration(offset), xalign=0, xpad=10)
                delete = Gtk.Button(label="delete")
                delete.connect("clicked", self.onDelete, fileinfo.name)
                data_grid.insert_row(rows)
                data_grid.attach(fn, 0, rows, 1, 1)
                data_grid.attach(offset, 1, rows, 1, 1)
                data_grid.attach(delete, 2, rows, 1, 1)
                rows += 1
            box = Gtk.Box(orientation="horizontal")
            play = Gtk.Button(label="play")
            play.connect("clicked", self.onPlay, index)
            box.pack_start(play, expand=False, fill=True, padding=0)
            whitelist = Gtk.Button(label="whitelist")
            whitelist.connect("clicked", self.onWhitelist, index)
            box.pack_start(whitelist, expand=False, fill=True, padding=0)
            box.pack_end(Gtk.Label(), expand=True, fill=True, padding=0)
            data_grid.insert_row(rows)
            data_grid.attach(box, 0, rows, 3, 1)
            rows += 1

        main_area = self.builder.get_object("main_area")
        main_area.add(data_grid)

        self.builder.connect_signals(self)

        window.show_all()

        Gtk.main()

    def onPlay(self, *args):
        index = args[1]
        procs = []
        for fileinfo, offset, _ in self.duplicates[index]:
            if os.access(fileinfo.name, os.R_OK):
                procs.append(subprocess.Popen(["ffplay", "-sn", "-ss",
                                               format_duration(offset),
                                               fileinfo.name],
                                              stdout=subprocess.DEVNULL,
                                              stderr=subprocess.DEVNULL))

        finished = not bool(procs)
        while not finished:
            for p in procs:
                if (0, 0) != os.waitpid(p.pid, os.WNOHANG):
                    finished = True
                    break
            time.sleep(0.2)

        for p in procs:
            try:
                p.kill()
            except Exception:
                pass

    def onWhitelist(self, *args):
        args[0].destroy()
        index = args[1]
        fnames = [d[0].name for d in self.duplicates[index]]
        whitelist(self.dbi, self.params, fnames)

    def onDelete(self, *args):
        fname = args[1]
        try:
            os.remove(fname)
            logging.info("Deleted %s", fname)
            for group in self.duplicates:
                for fileinfo, offset, label in group:
                    if fname == fileinfo.name:
                        label.set_markup("<span strikethrough='true'>%s</span>"
                                         % GLib.markup_escape_text(self.make_label(fileinfo)))
                        label.set_use_markup(True)
        except Exception as e:
            logging.error("Failed to delete %s: %s", fname, e)

    def onQuit(self, *args):
        Gtk.main_quit(*args)

    def onPurge(self, *args):
        switch = self.builder.get_object("delete_switch")
        handle_purge(self.dbi, self.params, switch.get_active())

    def make_label(self, fileinfo):
        return format_duration(fileinfo.duration) + " " + fileinfo.name


def handle_import(dbi, params):
    if params.dir:
        for fname in get_files(params.dir,
                               params.vidext.replace(" ", "").split(",")):
            import_file(dbi, params, fname)
    if params.file:
        import_file(dbi, params, params.file)


def handle_search(dbi, params):
    idx = Index(dbi, params)
    duplicates = idx.search()

    if len(duplicates) == 0:
        logging.info("No candidates found, giving up")
        return

    if params.searchres:
        with open(params.searchres, "w") as f:
            yaml.dump(duplicates, f, indent=2)

    if params.ui:
        ui = VidDupUI(dbi, params, duplicates)
        ui.start()
    else:
        for match in duplicates:
            logging.info("Group of %d files found", len(match))
            for row in match:
                fileinfo, offset = row
                logging.info("ffplay -ss %s '%s'",
                             format_duration(offset), fileinfo.name)


def fetch_duplicates(dbi, f):
    candidates = yaml.load(f, Loader=yaml.Loader)
    result = []
    for row in candidates:
        new_row = []
        for fi, pos in row:
            if os.access(fi.name, os.R_OK):
                new_row.append([fi, pos])
        if len(new_row) < 2:
            continue

        fids = [fi[0].fid for fi in new_row]
        fids.sort()
        is_whitelisted = True
        for id1, id2 in combinations(fids, 2):
            if not dbi.is_whitelisted(id1, id2):
                is_whitelisted = False
                break
        if not is_whitelisted:
            result.append(new_row)

    return result


def handle_searchres(dbi, params):
    with open(params.searchres) as f:
        duplicates = fetch_duplicates(dbi, f)

    if params.ui:
        ui = VidDupUI(dbi, params, duplicates)
        ui.start()
    else:
        for match in duplicates:
            logging.info("Group of %d files found", len(match))
            for row in match:
                fileinfo, offset = row
                logging.info("ffplay -ss %s '%s'", format_duration(offset), fileinfo.name)


def fixfilenames(args):
    for fname in get_files(args.dir, args.vidext.replace(" ", "").split(",")):
        try:
            fname.encode("utf8")
        except Exception:
            logging.warn("invalid filename %s", fname, exc_info=True)
            fn = fname.encode("utf8", "ignore")
            new_fname = fn.decode("utf8")
            logging.info("changed into %s", new_fname)
            os.rename(fname, new_fname)


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--purge",
                        default=False,
                        action="store_true",
                        help="Purge deleted files from database (dry run mode)")
    parser.add_argument("--delete",
                        default=False,
                        action="store_true",
                        help="Really delete from database in purge")
    parser.add_argument("--nice",
                        type=int,
                        help="Nice level for background operation, default 5",
                        default=5)
    parser.add_argument("--dir",
                        help="Import video hashes from directory and its subdirectories")
    parser.add_argument("--file",
                        help="Import video hashes for a single file")
    parser.add_argument("--refresh",
                        action="store_true",
                        help="Re-hash file but keep whitelistings intact",
                        default=False)
    parser.add_argument("--search",
                        action="store_true",
                        help="Search duplicates in database")
    parser.add_argument("--ignore_start",
                        type=int,
                        default=0,
                        help="Ignore search results startin in"
                        " the first seconds of a movie, default 0")
    parser.add_argument("--ignore_end",
                        type=int,
                        default=0,
                        help="Ignore search results starting in the last"
                        " seconds of a movie, default 0")
    parser.add_argument("--db",
                        help="SQLITE3 database file")
    parser.add_argument("--dbhost",
                        help="PostgreSQL database server host",
                        default="gateway")
    parser.add_argument("--dbport",
                        type=int,
                        help="PostgreSQL database server port",
                        default=5433)
    parser.add_argument("--dbname",
                        help="PostgreSQL database name",
                        default="viddup")
    parser.add_argument("--dbuser",
                        help="PostgreSQL database user name",
                        default="viddup")
    parser.add_argument("--dbpwd",
                        help="PostgreSQL database user password",
                        default="viddup")
    parser.add_argument("--indexlength",
                        default=10,
                        type=int,
                        help="Length of index in searches, smaller"
                        " values will produce more false positives, default 10")
    parser.add_argument("--scenelength",
                        default=300,
                        type=int,
                        help="Length in seconds of scenes to match, default 300")
    parser.add_argument("--radius",
                        default=3.0,
                        type=float,
                        help="Measure for acceptable index difference. Higher"
                        " values will result in more false positives, default 3.0")
    parser.add_argument("--ui",
                        action="store_true",
                        help="Launch ui after search")
    parser.add_argument("--searchres",
                        help="Filename of search result, used"
                        " in --search and --ui without --search")
    parser.add_argument("--step",
                        type=int,
                        default=1,
                        help="Step width for searching index, default 1")
    parser.add_argument("--whitelist",
                        nargs='+',
                        help="Whitelist a list of files")
    parser.add_argument("--knnlib",
                        default=ANN_LIBS[0],
                        help="knn library to use, choose " + ", ".join(ANN_LIBS))
    parser.add_argument("--vidext",
                        default=KNOWN_VID_TYPES_DEFAULT,
                        help="filename extensions to consider,"
                        " default %s" % KNOWN_VID_TYPES_DEFAULT)
    parser.add_argument("--fixduration",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--fixspeed",
                        default=False,
                        action="store_true",
                        help="Make search more robust for time-scaled videos, "
                        "increase indexlength to reduce number of false positives")
    parser.add_argument("--fixfilenames",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--debug",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)
    parser.add_argument("--profile",
                        default=False,
                        action="store_true",
                        help=argparse.SUPPRESS)

    params = parser.parse_args()

    try:
        nl = os.nice(0)
        nl = os.nice(max(params.nice - nl, 0))
        logging.info("Nice level %d", nl)
    except Exception:
        logging.info("Setting nice level not supported")

    dbi = get_db(params)

    if params.knnlib not in ANN_LIBS:
        logging.error("Unsupported knn library %s", params.knnlib)
        sys.exit(1)

    if params.profile:
        profile = cProfile.Profile()
        profile.enable()

    if params.fixfilenames:
        if params.dir:
            fixfilenames(params)
        else:
            logging.error("please set --dir option as well")

    if params.whitelist:
        whitelist(dbi, params)

    if params.purge:
        dbi.tidy_db()
        handle_purge(dbi, params)

    if params.dir or params.file:
        handle_import(dbi, params)

    if params.search:
        handle_search(dbi, params)
    elif params.searchres:
        handle_searchres(dbi, params)

    if params.profile:
        profile.create_stats()
        s = pstats.Stats(profile)
        s.sort_stats("cumulative")
        s.print_stats(50)
